- region: eu-frankfurt-1
  compartment_id: ${OCI_COMPARTMENT_ID}
  models:
    ondemand:
      - name: cohere.command-plus-latest
        model_id: cohere.command-plus-latest
        description: "delivers roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same."
        "tool_call": True,  
        "stream_tool_call": True,  

      - name: cohere.command-latest
        model_id: cohere.command-latest
        description: "delivers roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same."
        "tool_call": True,  
        "stream_tool_call": True,  
#      - name: cohere.command-r-16k
#        model_id: cohere.command-r-16k
#        description: "Optimized for conversational interaction and long context tasks. Ideal for text generation, summarization, translation, or text-based classification."

      - name: meta.llama-3.3-70b-instruct
        model_id: meta.llama-3.3-70b-instruct
        description: "Model has 70 billion parameters.Accepts text-only inputs and produces text-only outputs.Delivers better performance than both Llama 3.1 70B and Llama 3.2 90B for text tasks.Maximum prompt + response length 128,000 tokens for each run.For on-demand inferencing, the response length is capped at 4,000 tokens for each run."
        "tool_call": True,
        "stream_tool_call": True,
      
      - name: openai.gpt-oss-120b
        model_id: openai.gpt-oss-120b
        description: "gpt-oss-120b"
        "tool_call": True,  
        "stream_tool_call": True, 

      - name: openai.gpt-oss-20b
        model_id: openai.gpt-oss-20b
        description: "gpt-oss-20b"
        "tool_call": True,  
        "stream_tool_call": True,

    embedding:
      - name: cohere.embed-multilingual-v3.0
        model_id: cohere.embed-multilingual-v3.0
        description: "Cohere multilingual embedding model v3.0"
#      - name: meta.llama-3.2-11b-vision-instruct
#        model_id: meta.llama-3.2-11b-vision-instruct
#        description: "Model has 11 billion parameters.Dedicated mode only. (On-demand inferencing not available.) For dedicated inferencing, create a dedicated AI cluster and endpoint and host the model on the cluster.Context length 128,000 tokens.Maximum prompt + response length 128,000 tokens for each run.Multimodal support Input text and images and get a text output.English is the only supported language for the image plus text option.Multilingual option supported for the text only option."

#      - name: meta.llama-3.2-90b-vision-instruct
#        model_id: meta.llama-3.2-90b-vision-instruct
#        description: "Model has 90 billion parameters.Context length: 128,000 tokens.Maximum prompt + response length: 128,000 tokens for each run.For on-demand inferencing, the response length is capped at 4,000 tokens for each run.Multimodal support: Input text and images and get a text output.English is the only supported language for the image plus text option.Multilingual option supported for the text only option."

      #- name: meta.llama-3.1-405b-instruct
      #  model_id: meta.llama-3.1-405b-instruct
      #  description: "This 405 billion-parameter model is a high-performance option that offers speed and scalability."
      #  "tool_call": True,  
      #  "stream_tool_call": True, 

# - region: us-chicago-1
#   compartment_id: ocid1.compartment.oc1..
#   models:
#     ondemand:
#       - name: meta.llama-3.2-90b-vision-instruct
#         model_id: meta.llama-3.2-90b-vision-instruct
#         description: "Model has 11 billion parameters.Dedicated mode only. (On-demand inferencing not available.) For dedicated inferencing, create a dedicated AI cluster and endpoint and host the model on the cluster.Context length 128,000 tokens.Maximum prompt + response length 128,000 tokens for each run.Multimodal support Input text and images and get a text output.English is the only supported language for the image plus text option.Multilingual option supported for the text only option."
#         "tool_call": True,  
#         "stream_tool_call": True, 
#         stream_multimodal: True,
#         multimodal: True,

#    dedicated:
#      - name: my-dedicated-model-name
#        endpoint: https://ocid1.generativeaiendpoint....  # endpoint url for dedicated model
#        description: "my dedicated model description"

#    datascience:
#      - name: my-datascience-model-name
#        endpoint: https://modeldeployment.xxxxxx/predict  # Model deployment endpoint url
#        description: "my dedicated model description"

# Modify this file to specify the call information of the model.
# You can define 3 types of models:
# ondemand: pre-trained model on OCI generative AI
# dedicated: dedicated model on OCI generative AI, including dedicated infrastructure, fine-tuned model, etc.
# datascience: model deployed by OCI data science service

# Where:
# region: the region where the model is located
# compartment_id: the compartment where the model is located
# name: any specified model name, use this name to point to different models when calling
# model_id: for ondemand, it is the model id, for dedicated and datascience, it is the call endpoint
